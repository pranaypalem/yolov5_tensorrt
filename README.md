# YOLOv8 + TensorRT ROS Inference (Dockerized)

This repository contains a minimal and production-ready Docker environment to run **YOLOv8 inference** in real-time using **TensorRT**, integrated with **ROS (Noetic)** for robotics workflows. Built for Jetson or remote GPU inference, with CPU fallback.

---

## ğŸš€ Docker Image on Docker Hub

ğŸ“¦ [DockerHub: pranaypalem/yolov8_tensorrt](https://hub.docker.com/r/pranaypalem/yolov8_tensorrt)

```bash
docker pull pranaypalem/yolov8_tensorrt:3.1.3
```

---

## ğŸ”§ Features

- âœ… ROS Noetic (minimal setup)
- âœ… Inference via TensorRT engine (or ONNXRuntime as fallback)
- âœ… Subscribes to image topic from ROS
- âœ… Publishes annotated image with YOLO detections
- âœ… GPU + CPU fallback support
- âœ… Caches `.engine` file for fast reuse

---

## ğŸ› ï¸ Requirements

- Docker with NVIDIA GPU support
- ROS master running (Jetson or another device)
- YOLOv8 `best.pt` model file
- **TensorRT tar.gz installer must be manually downloaded**:  
  > Download TensorRT from: [NVIDIA Developer TensorRT](https://developer.nvidia.com/tensorrt)
  >
  > Place `TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-11.8.tar.gz` into the project directory before building or running.

---

## ğŸ§ª How to Run

1. **Launch the Docker container:**

```bash
docker run -it --gpus all --net=host \
  -e ROS_MASTER_URI=http://172.20.10.4:11311 \
  -e ROS_IP=172.20.10.6 \
  pranaypalem/yolov8_tensorrt:3.1.3
```

2. **Convert the YOLOv8 model to TensorRT Engine:**

Inside the container:

```bash
python3 convert_to_engine.py
```

This will convert your `.pt` model to an optimized `.engine` file.

3. **Run the YOLO ROS inference node:**

```bash
python3 yolo_ros_inference.py
```

This script subscribes to a ROS image topic, runs YOLOv8 inference using TensorRT, and publishes the annotated image back.

---

## ğŸ“ Folder Structure

```
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ bestsynthetic.pt
â”œâ”€â”€ coke_can_depth_processor.py
â”œâ”€â”€ convert_to_engine.py
â”œâ”€â”€ test_video.avi
â”œâ”€â”€ video_publisher.py
â”œâ”€â”€ yolo_ros_inference.py
â”œâ”€â”€ TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-11.8.tar.gz   # Must be manually downloaded
â””â”€â”€ README.md
```

---

## ğŸ“‚ Environment Variables

| Variable        | Description                          |
|------------------|--------------------------------------|
| `ROS_MASTER_URI` | IP address of the ROS master node    |
| `ROS_IP`         | Local machine IP to advertise to ROS |

---

## ğŸ§  Internals

- Converts `best.pt â†’ best.onnx â†’ best.engine`
- Runs YOLOv8 inference using TensorRT (or ONNXRuntime as fallback)
- Publishes annotated images back to ROS topics

---

## ğŸ§‘â€ğŸ’» Author

**Pranay Palem**  
Optimized for real-time robotics, computer vision pipelines, and GPU inference workflows.

---

## ğŸ“œ License

MIT License â€” use freely and responsibly.
